{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from diffmask.models.sentiment_classification_sst import (\n",
    "    BertSentimentClassificationSST,\n",
    "    MyDataset,\n",
    "    my_collate_fn,\n",
    "    my_collate_fn_rationale,\n",
    "    load_sst\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gpu\", type=str, default=\"0\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\n",
    "        \"--val_filename\",\n",
    "        type=str,\n",
    "        # default=\"datasets/eSNLI/esnli_test.csv\"\n",
    "        # default=\"datasets/coco_test/train_api.txt\"\n",
    "        default=\"datasets/sci_chatgpt/test_data/snli_1.0_dev_output_edit.txt\"\n",
    "        # default=\"datasets/sci_chatgpt/test_data/500_mnli_mis.txt\"\n",
    "        \n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--val_rationale\",\n",
    "        type=str,\n",
    "        default=\"datasets/eSNLI/esnli_test.rationale_idx\"\n",
    "        \n",
    ")\n",
    "    parser.add_argument(\n",
    "        \"--model_path\",\n",
    "        type=str,\n",
    "    \n",
    "        default=\"outputs/coco-bert-hjy_snli_ml_15rules_for_distribution_acc/epoch=14-val_acc=0.9637-val_f1=0.8513.ckpt\"\n",
    "        # default=\"outputs/coco-bert-hjy_snli_ml_19rules_for_distribution_acc/epoch=7-val_acc=0.9448-val_f1=0.7821.ckpt\"\n",
    "\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"coco\", # coco, esnli\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--token_cls\", type=bool, default=False, help=\"Enable token classification\")\n",
    "\n",
    "    hparams, _ = parser.parse_known_args()\n",
    "    \n",
    "    torch.manual_seed(hparams.seed)\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = hparams.gpu\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = BertSentimentClassificationSST.load_from_checkpoint(hparams.model_path).to(device)\n",
    "\n",
    "model.freeze()\n",
    "model.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Validation 데이터 로드\n",
    "val_dataset, _ = load_sst(\n",
    "    hparams.val_filename, None, hparams.dataset, model.hparams.num_labels, hparams.val_rationale, hparams.token_cls\n",
    ")\n",
    "\n",
    "# 데이터 필터링: 라벨이 '-'인 행 제거\n",
    "filtered_dataset = []\n",
    "skipped_count = 0\n",
    "\n",
    "for data in val_dataset:\n",
    "    label = data[1]  # 튜플의 두 번째 항목이 라벨로 가정\n",
    "    if isinstance(label, str) and label.strip() == '-':  # 라벨이 '-'인 경우\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    filtered_dataset.append(data)  # 유효한 데이터 추가\n",
    "\n",
    "print(f\"Skipped rows with '-' label: {skipped_count}\")\n",
    "\n",
    "# DataLoader 준비\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    filtered_dataset, batch_size=model.hparams.batch_size, collate_fn=my_collate_fn_rationale, num_workers=8\n",
    ")\n",
    "\n",
    "# 이후 코드는 기존 코드와 동일\n",
    "val_acc, num = [0]*15, [0]*15  # 각 라벨별 정확도\n",
    "# val_acc, num = [0]*19, [0]*19  # 각 라벨별 정확도\n",
    "results = []\n",
    "\n",
    "for i, batch in tqdm(enumerate(val_dataloader), total=len(filtered_dataset) // model.hparams.batch_size):\n",
    "    inputs = model.tokenizer.batch_encode_plus(batch[0], pad_to_max_length=True, return_tensors='pt').to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    labels = batch[1].to(device)\n",
    "\n",
    "    logits = model.forward(input_ids, mask, token_type_ids)[0]\n",
    "    predictions = logits.argmax(-1)  # 예측값\n",
    "\n",
    "    for logit, label, sentence in zip(predictions, labels, batch[0]):\n",
    "        val_acc[label] += (logit == label).int()\n",
    "        num[label] += 1\n",
    "\n",
    "        # 전제-가설-원본라벨-예측라벨 저장\n",
    "        premise, hypothesis = sentence\n",
    "        results.append({\n",
    "            \"Premise\": premise,\n",
    "            \"Hypothesis\": hypothesis,\n",
    "            \"Original Label\": label.item(),\n",
    "            \"Predicted Label\": logit.item()\n",
    "        })\n",
    "\n",
    "# 정확도 출력\n",
    "print(f\"Overall Accuracy: {sum(val_acc) / sum(num)}\")\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 라벨 역매핑 추가\n",
    "label_idx = {\n",
    "    'entailment_HS': 0, 'entailment_PS': 1, 'entailment_COUNT': 2, 'entailment_PA': 3, 'entailment_ES': 4,\n",
    "    'contradiction_CW_adj': 5, 'contradiction_CW_noun': 6, 'contradiction_CV': 7, 'contradiction_NS': 8,\n",
    "    'contradiction_SOS': 9, 'contradiction_IH': 10, 'contradiction_NI': 11,\n",
    "    'neutral_AM': 12, 'neutral_CON': 13, 'neutral_SSNCV': 14\n",
    "}\n",
    "\n",
    "# label_idx = {\n",
    "#     'entailment_HS': 0, 'entailment_PS': 1, 'entailment_COUNT': 2, 'entailment_PA': 3, 'entailment_ES': 4,\n",
    "#     'contradiction_CW_adj': 5, 'contradiction_CW_noun': 6, 'contradiction_CV': 7, 'contradiction_NS': 8,\n",
    "#     'contradiction_SOS': 9, 'contradiction_IH': 10, 'contradiction_NI': 11,\n",
    "#     'neutral_AM': 12, 'neutral_CON': 13, 'neutral_SSNCV': 14, 'neutral_CA': 15, 'neutral_EI' : 16, \n",
    "#             'entailment_RG': 17, 'neutral_VS' : 18\n",
    "# }\n",
    "\n",
    "reverse_label_idx = {v: k for k, v in label_idx.items()}\n",
    "\n",
    "# 라벨을 문자열로 변환\n",
    "results_df[\"Original Label\"] = results_df[\"Original Label\"].map(reverse_label_idx)\n",
    "results_df[\"Predicted Label\"] = results_df[\"Predicted Label\"].map(reverse_label_idx)\n",
    "\n",
    "# 엑셀 파일로 저장\n",
    "output_file = \"ml_outputs/devset_10000_15rules_distribution2.xlsx\"\n",
    "results_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
