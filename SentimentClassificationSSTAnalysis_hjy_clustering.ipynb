{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# clustering 코드1 - perturbation(word embedding x) 적용\n",
    "# 130 서버 gpu 1 사용중\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "\n",
    "#from tqdm.notebook import tqdm, trange\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from diffmask.models.sentiment_classification_sst_hhs import (\n",
    "    BertSentimentClassificationSST,\n",
    "    BertSentimentClassificationSST_hhs, # 미분(CLS 벡터)\n",
    "    BertSentimentClassificationSST_hhs2, # 드랍아웃 레이어\n",
    "    BertSentimentClassificationSST_hhs3, # 미분(word embedding)\n",
    "    BertSentimentClassificationSST_hjy, # cls embedding\n",
    "    MyDataset,\n",
    "    my_collate_fn,\n",
    "    my_collate_fn_rationale,\n",
    "    load_sst\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--gpu\", type=str, default=\"1\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0)\n",
    "    parser.add_argument(\n",
    "        \"--val_filename\",\n",
    "        type=str,\n",
    "        # default=\"datasets/eSNLI/esnli_test.csv\"\n",
    "        # default=\"datasets/coco_test/train_api.txt\"\n",
    "        # default=\"datasets/CoT/snli/clustering/split_1.txt\"\n",
    "        default=\"datasets/CoT/snli/snli_label_changed.txt\"\n",
    "        # default=\"datasets/sci_chatgpt/test_data/500_mnli_mis.txt\"\n",
    "        \n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--val_rationale\",\n",
    "        type=str,\n",
    "        default=\"datasets/eSNLI/esnli_test.rationale_idx\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--token_cls\", \n",
    "        action='store_true'\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--model_path\",\n",
    "        type=str,\n",
    "        default=\"outputs/coco-bert-hjy_snli_15label_new_split/epoch=11-val_acc=0.9656-val_f1=0.9414.ckpt\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"coco\", # coco, esnli\n",
    "    )\n",
    "\n",
    "    hparams, _ = parser.parse_known_args()\n",
    "    \n",
    "    # 환경 변수 설정\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = hparams.gpu\n",
    "\n",
    "    # GPU 할당\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "model = BertSentimentClassificationSST_hjy.load_from_checkpoint(hparams.model_path).to(device) # 내가 원하는 형태로 output만 뽑기\n",
    "model.hparams.model_path = hparams.model_path\n",
    "\n",
    "model.freeze()\n",
    "model.prepare_data()\n",
    "print(\"Current CUDA device index:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# softmax with temperature scaling 적용\n",
    "def softmax_with_temperature(logits, T):\n",
    "    logits = logits / T  # Temperature scaling\n",
    "    max_logits = torch.max(logits, dim=1, keepdim=True)[0]\n",
    "    exp_logits = torch.exp(logits - max_logits)\n",
    "    sum_exp_logits = torch.sum(exp_logits, dim=1, keepdim=True)\n",
    "    y = exp_logits / sum_exp_logits\n",
    "    return y\n",
    "\n",
    "T = 700 \n",
    "epsilon = 0.033\n",
    "probability_threshold = 0.071486\n",
    "\n",
    "# val_dataset 및 val_dataloader 초기화\n",
    "val_dataset, _ = load_sst(\n",
    "                hparams.val_filename, None, hparams.dataset, model.hparams.num_labels, hparams.val_rationale, model.hparams.token_cls\n",
    "            )\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "            val_dataset, batch_size=1, collate_fn=my_collate_fn_rationale, num_workers=8\n",
    "        )\n",
    "\n",
    "selected_clustering_values = []\n",
    "selected_clustering_indices = []\n",
    "selected_premises = []\n",
    "selected_hypotheses = []\n",
    "selected_cls_embeddings = []  # cls embedding을 저장할 리스트 추가\n",
    "\n",
    "# 입력데이터를 모델에 전달\n",
    "for i, batch in tqdm(enumerate(model.val_dataloader()), total=len(model.val_dataset) // model.hparams.batch_size): # Coco\n",
    "    model.eval()\n",
    "    inputs = model.tokenizer.batch_encode_plus(batch[0], pad_to_max_length=True, return_tensors='pt').to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    labels = batch[1].to(device)\n",
    "    \n",
    "    logits, new_logits, cls_embedding = model.forward(input_ids, mask, token_type_ids, temperature=T, epsilon=epsilon)  # cls_embedding 추가\n",
    "\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # new_logits에 대해 소프트맥스 함수를 적용\n",
    "    softmaxed_clustering_output = F.softmax(new_logits, dim=1)  # 이 코드로 진행\n",
    "    \n",
    "    max_values_clustering, max_indices_clustering = torch.max(softmaxed_clustering_output, dim=1)\n",
    "    \n",
    "    for j in range(len(max_values_clustering)):\n",
    "        if max_values_clustering[j] <= probability_threshold:\n",
    "            selected_clustering_indices.append(max_indices_clustering[j].detach().cpu().numpy())\n",
    "            selected_clustering_values.append(max_values_clustering[j].detach().cpu().numpy())\n",
    "            \n",
    "            # 원래의 문장으로 복원\n",
    "            premise, hypothesis = batch[0][j]\n",
    "            \n",
    "            selected_premises.append(premise.strip())\n",
    "            selected_hypotheses.append(hypothesis.strip())\n",
    "            \n",
    "            # cls embedding 추가\n",
    "            selected_cls_embeddings.append(cls_embedding[j].detach().cpu().numpy())  # CLS 토큰 임베딩 추가\n",
    "\n",
    "# 데이터 프레임 생성 및 엑셀로 저장\n",
    "results_df = pd.DataFrame({\n",
    "    'Indices': selected_clustering_indices,\n",
    "    'Probabilities': selected_clustering_values,\n",
    "    'Premise': selected_premises,\n",
    "    'Hypothesis': selected_hypotheses,\n",
    "    'CLS_Embedding': selected_cls_embeddings  # cls embedding 추가\n",
    "})\n",
    "\n",
    "# Excel 파일로 저장 (파일 경로에 따라 수정 필요)\n",
    "results_df.to_excel('/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/output_sort_51914_cls.xlsx', index=False)\n",
    "\n",
    "# 저장된 행의 개수 출력\n",
    "num_selected_rows = len(selected_clustering_values)\n",
    "print(f\"Number of rows saved: {num_selected_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 여기부터 돌려보기 걸리는 시간 5000개 기준 36s\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 엑셀파일 경로\n",
    "cluster_output_dir = '/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/cluster_output_5000/'\n",
    "\n",
    "# 모든 클러스터링 된 엑셀 파일 읽기\n",
    "cluster_files = glob.glob(os.path.join(cluster_output_dir, 'output_cluster_*.xlsx'))\n",
    "\n",
    "# 각 클러스터 파일에 대해 'Indices'의 빈도수 계산 및 결과 저장\n",
    "results = []\n",
    "for file in cluster_files:\n",
    "    df = pd.read_excel(file)\n",
    "    index_counts = df['Indices'].value_counts()\n",
    "    most_common_index = index_counts.idxmax()\n",
    "    most_common_index_count = index_counts.max()\n",
    "    total_indices = len(df)\n",
    "    most_common_index_ratio = most_common_index_count / total_indices * 100\n",
    "    \n",
    "    results.append({\n",
    "        'file' : os.path.basename(file),\n",
    "        'most_common_index' : most_common_index,\n",
    "        'most_common_index_count' : most_common_index_count,\n",
    "        'total_indices' : total_indices,\n",
    "        'most_common_index_ratio' : most_common_index_ratio\n",
    "    })\n",
    "    \n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 출력\n",
    "print(results_df)\n",
    "\n",
    "# 빈도수가 높은 순으로 정렬\n",
    "results_df = results_df.sort_values(by='most_common_index_ratio', ascending=False)\n",
    "\n",
    "# 결과를 엑셀 파일로 저장\n",
    "results_df.to_excel('/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/cluster_analysis_5000_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# 엑셀 파일 불러오기\n",
    "input_file = '/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/output_sort_51914_cls.xlsx'\n",
    "results_df = pd.read_excel(input_file)\n",
    "\n",
    "# CLS 임베딩 데이터 전처리 함수\n",
    "def preprocess_embedding(embedding_str):\n",
    "    # 쉼표를 추가하여 올바른 리스트 형식으로 변환\n",
    "    embedding_str = re.sub(r'(?<=\\d)\\s+(?=-?\\d)', ', ', embedding_str.strip())\n",
    "    return np.array(eval(embedding_str))\n",
    "\n",
    "# CLS 임베딩 데이터 추출 및 전처리\n",
    "cls_embeddings = results_df['CLS_Embedding'].apply(preprocess_embedding).tolist()\n",
    "cls_embeddings = np.stack(cls_embeddings)\n",
    "\n",
    "# K-means 클러스터링 적용\n",
    "num_clusters = 1000  # 원하는 클러스터 개수 설정\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "clusters = kmeans.fit_predict(cls_embeddings)\n",
    "\n",
    "# 클러스터 정보 추가\n",
    "results_df['Cluster'] = clusters\n",
    "\n",
    "# 클러스터별 문장쌍 추출 및 엑셀 파일로 저장\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_df = results_df[results_df['Cluster'] == cluster_num]\n",
    "    cluster_df.to_excel(f'/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/cluster_output_1000/output_cluster_{cluster_num}.xlsx', index=False)\n",
    "\n",
    "# 각 클러스터에 속하는 문장 쌍의 개수를 계산\n",
    "cluster_sizes = results_df['Cluster'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# 각 클러스터에 속하는 문장 쌍의 개수를 출력 (문장 수 기준으로 정렬)\n",
    "print(f\"Number of clusters: {num_clusters}\")\n",
    "for cluster_num, size in cluster_sizes.iteritems():\n",
    "    print(f\"Cluster {cluster_num}: {size} rows\")\n",
    "\n",
    "# t-SNE 결과를 사용하여 2차원으로 축소\n",
    "perplexity = min(30, len(cls_embeddings) // 2)\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=perplexity)\n",
    "tsne_result = tsne.fit_transform(cls_embeddings)\n",
    "\n",
    "# t-SNE 결과를 데이터프레임에 추가\n",
    "results_df['tsne-2d-one'] = tsne_result[:, 0]\n",
    "results_df['tsne-2d-two'] = tsne_result[:, 1]\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(16,10))\n",
    "scatter = plt.scatter(results_df['tsne-2d-one'], results_df['tsne-2d-two'], c=results_df['Cluster'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# # 클러스터 중심에 클러스터 번호 추가 (옵션)\n",
    "# cluster_centers = kmeans.cluster_centers_\n",
    "# tsne_cluster_centers = tsne.fit_transform(cluster_centers)\n",
    "# for i, txt in enumerate(range(num_clusters)):\n",
    "#     plt.annotate(txt, (tsne_cluster_centers[i, 0], tsne_cluster_centers[i, 1]), fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.title('t-SNE visualization of CLS Embeddings with KMeans Clustering')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# 엑셀 파일 불러오기\n",
    "input_file = '/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/output_sort_51914_cls.xlsx'\n",
    "results_df = pd.read_excel(input_file)\n",
    "\n",
    "# CLS 임베딩 데이터 전처리 함수\n",
    "def preprocess_embedding(embedding_str):\n",
    "    # 쉼표를 추가하여 올바른 리스트 형식으로 변환\n",
    "    embedding_str = re.sub(r'(?<=\\d)\\s+(?=-?\\d)', ', ', embedding_str.strip())\n",
    "    return np.array(eval(embedding_str))\n",
    "\n",
    "# CLS 임베딩 데이터 추출 및 전처리\n",
    "cls_embeddings = results_df['CLS_Embedding'].apply(preprocess_embedding).tolist()\n",
    "cls_embeddings = np.stack(cls_embeddings)\n",
    "\n",
    "# K-means 클러스터링 적용\n",
    "num_clusters = 5000  # 원하는 클러스터 개수 설정\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "clusters = kmeans.fit_predict(cls_embeddings)\n",
    "\n",
    "# 클러스터 정보 추가\n",
    "results_df['Cluster'] = clusters\n",
    "\n",
    "# 클러스터별 문장쌍 추출 및 엑셀 파일로 저장\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_df = results_df[results_df['Cluster'] == cluster_num]\n",
    "    cluster_df.to_excel(f'/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/cluster_output_5000/output_cluster_{cluster_num}.xlsx', index=False)\n",
    "\n",
    "# 각 클러스터에 속하는 문장 쌍의 개수를 계산\n",
    "cluster_sizes = results_df['Cluster'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# 각 클러스터에 속하는 문장 쌍의 개수를 출력 (문장 수 기준으로 정렬)\n",
    "print(f\"Number of clusters: {num_clusters}\")\n",
    "for cluster_num, size in cluster_sizes.iteritems():\n",
    "    print(f\"Cluster {cluster_num}: {size} rows\")\n",
    "\n",
    "# t-SNE 결과를 사용하여 2차원으로 축소\n",
    "perplexity = min(30, len(cls_embeddings) // 2)\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=perplexity)\n",
    "tsne_result = tsne.fit_transform(cls_embeddings)\n",
    "\n",
    "# t-SNE 결과를 데이터프레임에 추가\n",
    "results_df['tsne-2d-one'] = tsne_result[:, 0]\n",
    "results_df['tsne-2d-two'] = tsne_result[:, 1]\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(16,10))\n",
    "scatter = plt.scatter(results_df['tsne-2d-one'], results_df['tsne-2d-two'], c=results_df['Cluster'], cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "\n",
    "# # 클러스터 중심에 클러스터 번호 추가 (옵션)\n",
    "# cluster_centers = kmeans.cluster_centers_\n",
    "# tsne_cluster_centers = tsne.fit_transform(cluster_centers)\n",
    "# for i, txt in enumerate(range(num_clusters)):\n",
    "#     plt.annotate(txt, (tsne_cluster_centers[i, 0], tsne_cluster_centers[i, 1]), fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.title('t-SNE visualization of CLS Embeddings with KMeans Clustering')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## 여기부터 돌려보기 걸리는 시간 5000개 기준 36s\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 엑셀파일 경로\n",
    "cluster_output_dir = '/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/cluster_output_10000/'\n",
    "\n",
    "# 모든 클러스터링 된 엑셀 파일 읽기\n",
    "cluster_files = glob.glob(os.path.join(cluster_output_dir, 'output_cluster_*.xlsx'))\n",
    "\n",
    "# 각 클러스터 파일에 대해 'Indices'의 빈도수 계산 및 결과 저장\n",
    "results = []\n",
    "for file in cluster_files:\n",
    "    df = pd.read_excel(file)\n",
    "    index_counts = df['Indices'].value_counts()\n",
    "    most_common_index = index_counts.idxmax()\n",
    "    most_common_index_count = index_counts.max()\n",
    "    total_indices = len(df)\n",
    "    most_common_index_ratio = most_common_index_count / total_indices * 100\n",
    "    \n",
    "    results.append({\n",
    "        'file' : os.path.basename(file),\n",
    "        'most_common_index' : most_common_index,\n",
    "        'most_common_index_count' : most_common_index_count,\n",
    "        'total_indices' : total_indices,\n",
    "        'most_common_index_ratio' : most_common_index_ratio\n",
    "    })\n",
    "    \n",
    "# 결과를 데이터프레임으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 출력\n",
    "print(results_df)\n",
    "\n",
    "# 빈도수가 높은 순으로 정렬\n",
    "results_df = results_df.sort_values(by='most_common_index_ratio', ascending=False)\n",
    "\n",
    "# 결과를 엑셀 파일로 저장\n",
    "results_df.to_excel('/home/hjy/Rationale_Extraction_using_Diffmask-main/Rationale_Extraction_using_Diffmask-main/clustering/cluster_analysis_10000_results.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
